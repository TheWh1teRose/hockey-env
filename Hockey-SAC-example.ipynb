{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f5b8037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from SAC.replay_buffer import PrioritizedReplayBuffer\n",
    "from SAC.SAC import SAC\n",
    "from SAC.recorder import WandBRecorder\n",
    "import hockey.hockey_env as h_env\n",
    "from SAC.helpers import normalize_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7aa002",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: [wandb.login()] Loaded credentials for https://api.wandb.ai from /home/felix/.netrc.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfelix-loos\u001b[0m (\u001b[33mhtwk-robots\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.24.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/felix/Studium/Reinforcement-Learning/exam/hockey-env/wandb/run-20260118_124857-uoco747p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/htwk-robots/hockey-sac/runs/uoco747p' target=\"_blank\">shooting_training</a></strong> to <a href='https://wandb.ai/htwk-robots/hockey-sac' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/htwk-robots/hockey-sac' target=\"_blank\">https://wandb.ai/htwk-robots/hockey-sac</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/htwk-robots/hockey-sac/runs/uoco747p' target=\"_blank\">https://wandb.ai/htwk-robots/hockey-sac/runs/uoco747p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box(-inf, inf, (18,), float32)\n",
      "Action space: Box(-1.0, 1.0, (8,), float32)\n",
      "Action space bounds: [-1.0, 1.0]\n",
      "WandB run URL: https://wandb.ai/htwk-robots/hockey-sac/runs/uoco747p\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "config = {\n",
    "    # Environment settings\n",
    "    \"environment\": {\n",
    "        \"env_mode\": \"TRAIN_SHOOTING\",\n",
    "    },\n",
    "    # Training settings\n",
    "    \"training\": {\n",
    "        \"num_episodes\": 1000,\n",
    "        \"episode_length\": 1000,\n",
    "        \"checkpoint_freq\": 50,\n",
    "        \"device\": \"cpu\",\n",
    "    },\n",
    "    # SAC hyperparameters\n",
    "    \"sac\": {\n",
    "        \"hidden_dim\": 128,\n",
    "        \"lr\": 0.001,\n",
    "        \"gamma\": 0.99,\n",
    "        \"tau\": 0.01,\n",
    "        \"alpha\": 0.2,\n",
    "    },\n",
    "    # Replay buffer settings\n",
    "    \"buffer\": {\n",
    "        \"size\": 100000,\n",
    "        \"batch_size\": 100,\n",
    "        \"min_size\": 1000,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Create the Hockey environment\n",
    "env = h_env.HockeyEnv(mode=h_env.Mode.TRAIN_SHOOTING)\n",
    "\n",
    "buffer = PrioritizedReplayBuffer(config[\"buffer\"][\"size\"], env.observation_space.shape[0], 4)\n",
    "sac = SAC(buffer, env.observation_space.shape[0], 4, config[\"sac\"][\"hidden_dim\"], \n",
    "          config[\"sac\"][\"lr\"], config[\"sac\"][\"gamma\"], config[\"sac\"][\"tau\"], \n",
    "          config[\"sac\"][\"alpha\"], config[\"training\"][\"device\"])\n",
    "\n",
    "# Initialize WandB Recorder\n",
    "recorder = WandBRecorder(\n",
    "    project=\"hockey-sac\",\n",
    "    config=config,\n",
    "    run_name=\"shooting_training\",\n",
    "    checkpoint_dir=\"checkpoints/shooting\",\n",
    "    checkpoint_freq=config[\"training\"][\"checkpoint_freq\"],\n",
    "    save_best=True,\n",
    "    tags=[\"shooting\", \"sac\"],\n",
    "    notes=\"SAC training on Hockey shooting mode\",\n",
    ")\n",
    "\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Action space bounds: [{env.action_space.low[0]}, {env.action_space.high[0]}]\")\n",
    "print(f\"WandB run URL: {recorder.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ae12f4",
   "metadata": {},
   "source": [
    "## Shooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18d69c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felix/Studium/Reinforcement-Learning/exam/hockey-env/venv/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10 | Reward: -24.10 | Steps: 717\n",
      "Episode 20 | Reward: -24.27 | Steps: 1427\n",
      "Episode 30 | Reward: 6.76 | Steps: 2102\n",
      "Episode 40 | Reward: -5.17 | Steps: 2710\n",
      "Checkpoint saved: checkpoints/shooting/checkpoint_step_50.pt (uploaded to WandB)\n",
      "Episode 50 | Reward: -14.33 | Steps: 3290\n",
      "Episode 60 | Reward: 7.95 | Steps: 3759\n",
      "Episode 70 | Reward: 8.91 | Steps: 4287\n",
      "Episode 80 | Reward: -18.17 | Steps: 4876\n",
      "Episode 90 | Reward: -15.75 | Steps: 5403\n",
      "Checkpoint saved: checkpoints/shooting/checkpoint_step_100.pt (uploaded to WandB)\n",
      "Episode 100 | Reward: -18.18 | Steps: 5997\n",
      "Episode 110 | Reward: -22.33 | Steps: 6654\n",
      "Episode 120 | Reward: -7.95 | Steps: 7327\n",
      "Episode 130 | Reward: 8.84 | Steps: 7720\n",
      "Episode 140 | Reward: 7.34 | Steps: 8250\n",
      "Checkpoint saved: checkpoints/shooting/checkpoint_step_150.pt (uploaded to WandB)\n",
      "Episode 150 | Reward: -9.34 | Steps: 8765\n",
      "Episode 160 | Reward: -15.23 | Steps: 9392\n",
      "Episode 170 | Reward: -5.54 | Steps: 9958\n",
      "Episode 180 | Reward: -9.57 | Steps: 10545\n",
      "Episode 190 | Reward: -4.75 | Steps: 11042\n",
      "Checkpoint saved: checkpoints/shooting/checkpoint_step_200.pt (uploaded to WandB)\n",
      "Episode 200 | Reward: -3.71 | Steps: 11576\n",
      "Episode 210 | Reward: 5.55 | Steps: 12017\n",
      "Episode 220 | Reward: -0.90 | Steps: 12709\n",
      "Episode 230 | Reward: -17.55 | Steps: 13214\n",
      "Episode 240 | Reward: -15.51 | Steps: 13599\n",
      "Checkpoint saved: checkpoints/shooting/checkpoint_step_250.pt (uploaded to WandB)\n",
      "Episode 250 | Reward: 8.78 | Steps: 14152\n",
      "Episode 260 | Reward: -9.56 | Steps: 14690\n",
      "Episode 270 | Reward: -7.79 | Steps: 15167\n",
      "Episode 280 | Reward: -5.60 | Steps: 15643\n",
      "Episode 290 | Reward: 8.09 | Steps: 16032\n",
      "Checkpoint saved: checkpoints/shooting/checkpoint_step_300.pt (uploaded to WandB)\n",
      "Episode 300 | Reward: -7.74 | Steps: 16740\n",
      "Episode 310 | Reward: -4.21 | Steps: 17364\n",
      "Episode 320 | Reward: -8.36 | Steps: 18017\n",
      "Episode 330 | Reward: -20.54 | Steps: 18431\n",
      "Episode 340 | Reward: 8.69 | Steps: 19046\n",
      "Checkpoint saved: checkpoints/shooting/checkpoint_step_350.pt (uploaded to WandB)\n",
      "Episode 350 | Reward: 8.63 | Steps: 19526\n",
      "Episode 360 | Reward: -8.82 | Steps: 20116\n",
      "Episode 370 | Reward: 8.33 | Steps: 20658\n",
      "Episode 380 | Reward: 8.22 | Steps: 21034\n",
      "Episode 390 | Reward: -15.22 | Steps: 21537\n",
      "Checkpoint saved: checkpoints/shooting/checkpoint_step_400.pt (uploaded to WandB)\n",
      "Episode 400 | Reward: 9.04 | Steps: 21970\n",
      "Episode 410 | Reward: 7.03 | Steps: 22612\n",
      "Episode 420 | Reward: 7.75 | Steps: 23033\n",
      "Episode 430 | Reward: -2.83 | Steps: 23503\n",
      "Episode 440 | Reward: 7.36 | Steps: 23844\n",
      "Checkpoint saved: checkpoints/shooting/checkpoint_step_450.pt (uploaded to WandB)\n",
      "Episode 450 | Reward: 5.33 | Steps: 24177\n",
      "Episode 460 | Reward: 8.23 | Steps: 24691\n",
      "Episode 470 | Reward: -14.14 | Steps: 25131\n",
      "Episode 480 | Reward: -18.76 | Steps: 25612\n",
      "Episode 490 | Reward: 7.57 | Steps: 26060\n",
      "Checkpoint saved: checkpoints/shooting/checkpoint_step_500.pt (uploaded to WandB)\n",
      "Episode 500 | Reward: -15.52 | Steps: 26537\n",
      "Episode 510 | Reward: 8.78 | Steps: 26832\n",
      "Episode 520 | Reward: 8.63 | Steps: 27238\n",
      "Episode 530 | Reward: 7.83 | Steps: 27665\n",
      "Episode 540 | Reward: -7.83 | Steps: 28046\n",
      "Checkpoint saved: checkpoints/shooting/checkpoint_step_550.pt (uploaded to WandB)\n",
      "Episode 550 | Reward: 8.71 | Steps: 28367\n",
      "Episode 560 | Reward: 10.00 | Steps: 28681\n",
      "Episode 570 | Reward: 7.87 | Steps: 28996\n",
      "Episode 580 | Reward: -8.47 | Steps: 29413\n",
      "Episode 590 | Reward: 7.46 | Steps: 29794\n",
      "Checkpoint saved: checkpoints/shooting/checkpoint_step_600.pt (uploaded to WandB)\n",
      "Episode 600 | Reward: -1.83 | Steps: 30198\n",
      "Episode 610 | Reward: -6.21 | Steps: 30636\n",
      "Episode 620 | Reward: 7.49 | Steps: 30893\n",
      "Episode 630 | Reward: -12.00 | Steps: 31323\n",
      "Episode 640 | Reward: 8.96 | Steps: 31731\n",
      "Checkpoint saved: checkpoints/shooting/checkpoint_step_650.pt (uploaded to WandB)\n",
      "Episode 650 | Reward: 4.37 | Steps: 32034\n",
      "Episode 660 | Reward: -14.08 | Steps: 32414\n",
      "Episode 670 | Reward: 8.88 | Steps: 32710\n",
      "Episode 680 | Reward: -18.09 | Steps: 33061\n",
      "Episode 690 | Reward: 6.61 | Steps: 33396\n",
      "Checkpoint saved: checkpoints/shooting/checkpoint_step_700.pt (uploaded to WandB)\n",
      "Episode 700 | Reward: 7.99 | Steps: 33592\n",
      "Episode 710 | Reward: 8.50 | Steps: 33860\n",
      "Episode 720 | Reward: 8.09 | Steps: 34114\n",
      "Episode 730 | Reward: -15.65 | Steps: 34551\n",
      "Episode 740 | Reward: 8.59 | Steps: 34839\n",
      "Checkpoint saved: checkpoints/shooting/checkpoint_step_750.pt (uploaded to WandB)\n",
      "Episode 750 | Reward: -20.24 | Steps: 35285\n",
      "Episode 760 | Reward: -5.39 | Steps: 35620\n",
      "Episode 770 | Reward: 8.53 | Steps: 36036\n",
      "Episode 780 | Reward: 6.52 | Steps: 36381\n",
      "Episode 790 | Reward: 8.55 | Steps: 36762\n",
      "Checkpoint saved: checkpoints/shooting/checkpoint_step_800.pt (uploaded to WandB)\n",
      "Episode 800 | Reward: 8.15 | Steps: 37144\n",
      "Episode 810 | Reward: 8.73 | Steps: 37451\n",
      "Episode 820 | Reward: 7.43 | Steps: 37868\n",
      "Episode 830 | Reward: 9.02 | Steps: 38123\n",
      "Episode 840 | Reward: 7.65 | Steps: 38438\n",
      "Checkpoint saved: checkpoints/shooting/checkpoint_step_850.pt (uploaded to WandB)\n",
      "Episode 850 | Reward: 7.93 | Steps: 38816\n",
      "Episode 860 | Reward: 7.79 | Steps: 39065\n",
      "Episode 870 | Reward: 8.25 | Steps: 39389\n",
      "Episode 880 | Reward: 8.15 | Steps: 39608\n",
      "Episode 890 | Reward: -13.32 | Steps: 39948\n",
      "Checkpoint saved: checkpoints/shooting/checkpoint_step_900.pt (uploaded to WandB)\n",
      "Episode 900 | Reward: 7.85 | Steps: 40242\n",
      "Episode 910 | Reward: 8.50 | Steps: 40632\n",
      "Episode 920 | Reward: 9.07 | Steps: 40891\n",
      "Episode 930 | Reward: 7.07 | Steps: 41221\n",
      "Episode 940 | Reward: 6.96 | Steps: 41539\n",
      "Checkpoint saved: checkpoints/shooting/checkpoint_step_950.pt (uploaded to WandB)\n",
      "Episode 950 | Reward: 8.88 | Steps: 41881\n",
      "Episode 960 | Reward: 8.48 | Steps: 42241\n",
      "Episode 970 | Reward: 8.90 | Steps: 42504\n",
      "Episode 980 | Reward: 9.06 | Steps: 42706\n",
      "Episode 990 | Reward: 10.00 | Steps: 42955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved: checkpoints/shooting/checkpoint_step_1000.pt (uploaded to WandB)\n",
      "Episode 1000 | Reward: -1.69 | Steps: 43279\n",
      "Finishing WandB run: shooting_training\n",
      "Total episodes: 1000\n",
      "Total steps: 43279\n",
      "Best rolling reward: 5.34\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>buffer/capacity</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>buffer/fill_ratio</td><td>▁▁▁▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>buffer/size</td><td>▁▁▁▁▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇██████</td></tr><tr><td>episode</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>episode/closeness_to_puck</td><td>▁▄▄█▇▆▆▅▇███▇█▇▇█▇██▇▇▅██████████▇████▇█</td></tr><tr><td>episode/length</td><td>███▁▁██▁▅█▂▂██▁▄█▂▆▂▂▂▁▅█▂██▄▂▂▃▂▁█▂▂▂▁▂</td></tr><tr><td>episode/puck_direction</td><td>▃▃▃█▇▇▂▇▃▇▁▄▃▅▆▆▇▂▇▃▇▃▇▇▇▇▂▇▇▇▇▇▇▇▇▇▆▇▇▇</td></tr><tr><td>episode/reward</td><td>▁█▃█▅▄▄▆██▆▆▅▆▄▅▃▃▃█▅▄▃████▅█▅████▃█▅███</td></tr><tr><td>episode/rolling_reward</td><td>▁▂▂▃▃▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████████████</td></tr><tr><td>episode/rolling_win_rate</td><td>▁▁▃▃▃▄▄▄▅▅▄▄▄▄▅▅▆▆▆▆▇▇▇▇▇▇▇███▇█▇███████</td></tr><tr><td>+13</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>buffer/capacity</td><td>100000</td></tr><tr><td>buffer/fill_ratio</td><td>0.43279</td></tr><tr><td>buffer/size</td><td>43279</td></tr><tr><td>episode</td><td>1000</td></tr><tr><td>episode/closeness_to_puck</td><td>-1.68896</td></tr><tr><td>episode/length</td><td>81</td></tr><tr><td>episode/puck_direction</td><td>0.06396</td></tr><tr><td>episode/reward</td><td>-1.68896</td></tr><tr><td>episode/rolling_reward</td><td>5.16992</td></tr><tr><td>episode/rolling_win_rate</td><td>0.84</td></tr><tr><td>+13</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">shooting_training</strong> at: <a href='https://wandb.ai/htwk-robots/hockey-sac/runs/uoco747p' target=\"_blank\">https://wandb.ai/htwk-robots/hockey-sac/runs/uoco747p</a><br> View project at: <a href='https://wandb.ai/htwk-robots/hockey-sac' target=\"_blank\">https://wandb.ai/htwk-robots/hockey-sac</a><br>Synced 4 W&B file(s), 0 media file(s), 40 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260118_124857-uoco747p/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "obs, info = env.reset(seed=42)\n",
    "obs = normalize_obs(obs)\n",
    "\n",
    "global_step = 0\n",
    "total_reward = 0\n",
    "\n",
    "try:\n",
    "    for episode in range(config[\"training\"][\"num_episodes\"]):\n",
    "        episode_length = 0\n",
    "        last_info = info\n",
    "        \n",
    "        # Accumulators for training metrics\n",
    "        episode_metrics = {\n",
    "            \"actor_loss\": [],\n",
    "            \"critic_loss\": [],\n",
    "            \"alpha_loss\": [],\n",
    "            \"alpha\": [],\n",
    "            \"q1_mean\": [],\n",
    "            \"q2_mean\": [],\n",
    "        }\n",
    "        \n",
    "        # Accumulators for environment info rewards\n",
    "        episode_info_rewards = {\n",
    "            \"reward_closeness_to_puck\": 0.0,\n",
    "            \"reward_touch_puck\": 0.0,\n",
    "            \"reward_puck_direction\": 0.0,\n",
    "        }\n",
    "        \n",
    "        for step in range(config[\"training\"][\"episode_length\"]):\n",
    "            global_step += 1\n",
    "            episode_length += 1\n",
    "            \n",
    "            env.render()\n",
    "\n",
    "            # Sample action from SAC agent\n",
    "            action_1 = sac.act(obs)\n",
    "            action_2 = np.array([0, 0, 0, 0])\n",
    "            env_action = np.hstack([action_1, action_2])\n",
    "\n",
    "            # Take a step in the environment\n",
    "            next_obs, reward, terminated, truncated, info = env.step(env_action)\n",
    "            next_obs = normalize_obs(next_obs)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Accumulate info rewards\n",
    "            for key in episode_info_rewards:\n",
    "                if key in info:\n",
    "                    episode_info_rewards[key] += info[key]\n",
    "\n",
    "            buffer.add(obs, action_1, reward, next_obs, terminated)\n",
    "            obs = next_obs\n",
    "\n",
    "            # Update SAC and accumulate metrics\n",
    "            if buffer.is_ready(config[\"buffer\"][\"min_size\"]):\n",
    "                metrics = sac.update()\n",
    "                for key in episode_metrics:\n",
    "                    episode_metrics[key].append(metrics[key])\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                last_info = info\n",
    "                break\n",
    "\n",
    "        # Log accumulated training metrics at end of episode\n",
    "        if episode_metrics[\"actor_loss\"]:  # Only log if we had updates\n",
    "            recorder.log_update(\n",
    "                global_step=global_step,\n",
    "                actor_loss=np.mean(episode_metrics[\"actor_loss\"]),\n",
    "                critic_loss=np.mean(episode_metrics[\"critic_loss\"]),\n",
    "                alpha_loss=np.mean(episode_metrics[\"alpha_loss\"]),\n",
    "                alpha=np.mean(episode_metrics[\"alpha\"]),\n",
    "                q1_mean=np.mean(episode_metrics[\"q1_mean\"]),\n",
    "                q2_mean=np.mean(episode_metrics[\"q2_mean\"]),\n",
    "                extra_metrics={\n",
    "                    \"num_updates\": len(episode_metrics[\"actor_loss\"]),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # Log episode metrics with accumulated info rewards\n",
    "        is_best = recorder.log_episode(\n",
    "            episode=episode + 1,\n",
    "            reward=total_reward,\n",
    "            length=episode_length,\n",
    "            winner=last_info.get(\"winner\", 0),\n",
    "            info=episode_info_rewards,  # Pass accumulated rewards instead of last_info\n",
    "        )\n",
    "        \n",
    "        # Log buffer stats periodically\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            recorder.log_buffer(global_step, len(buffer), buffer.capacity)\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if (episode + 1) % config[\"training\"][\"checkpoint_freq\"] == 0:\n",
    "            recorder.save_checkpoint(sac, episode + 1)\n",
    "        \n",
    "        # Print progress\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            print(f\"Episode {episode + 1} | Reward: {total_reward:.2f} | Steps: {global_step}\")\n",
    "        \n",
    "        # Reset for next episode\n",
    "        total_reward = 0\n",
    "        obs, info = env.reset()\n",
    "\n",
    "finally:\n",
    "    # Always finish the recorder (uploads remaining data)\n",
    "    recorder.finish()\n",
    "    print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3adad36",
   "metadata": {},
   "source": [
    "## Handcrafed Opponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3ab778",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: [wandb.login()] Loaded credentials for https://api.wandb.ai from /home/felix/.netrc.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfelix-loos\u001b[0m (\u001b[33mhtwk-robots\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.24.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/felix/Studium/Reinforcement-Learning/exam/hockey-env/wandb/run-20260118_130919-10touplv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/htwk-robots/hockey-sac/runs/10touplv' target=\"_blank\">opponent_training</a></strong> to <a href='https://wandb.ai/htwk-robots/hockey-sac' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/htwk-robots/hockey-sac' target=\"_blank\">https://wandb.ai/htwk-robots/hockey-sac</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/htwk-robots/hockey-sac/runs/10touplv' target=\"_blank\">https://wandb.ai/htwk-robots/hockey-sac/runs/10touplv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box(-inf, inf, (18,), float32)\n",
      "Action space: Box(-1.0, 1.0, (8,), float32)\n",
      "Action space bounds: [-1.0, 1.0]\n",
      "WandB run URL: https://wandb.ai/htwk-robots/hockey-sac/runs/10touplv\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for opponent training\n",
    "config_opponent = {\n",
    "    \"env_mode\": \"NORMAL\",\n",
    "    \"opponent\": \"BasicOpponent_strong\",\n",
    "    \"buffer_size\": 100000,\n",
    "    \"hidden_dim\": 128,\n",
    "    \"lr\": 0.001,\n",
    "    \"gamma\": 0.99,\n",
    "    \"tau\": 0.01,\n",
    "    \"alpha\": 0.2,\n",
    "    \"batch_size\": 100,\n",
    "    \"min_buffer_size\": 1000,\n",
    "    \"num_episodes\": 1000,\n",
    "    \"max_steps\": 1000,\n",
    "    \"checkpoint_freq\": 50,\n",
    "}\n",
    "\n",
    "# Create the Hockey environment\n",
    "env = h_env.HockeyEnv()\n",
    "\n",
    "buffer = PrioritizedReplayBuffer(config_opponent[\"buffer_size\"], env.observation_space.shape[0], 4)\n",
    "sac = SAC(buffer, env.observation_space.shape[0], 4, config_opponent[\"hidden_dim\"], \n",
    "          config_opponent[\"lr\"], config_opponent[\"gamma\"], config_opponent[\"tau\"], \n",
    "          config_opponent[\"alpha\"], \"cpu\")\n",
    "\n",
    "player2 = h_env.BasicOpponent(weak=False)\n",
    "\n",
    "# Initialize WandB Recorder\n",
    "recorder = WandBRecorder(\n",
    "    project=\"hockey-sac\",\n",
    "    config=config_opponent,\n",
    "    run_name=\"opponent_training\",\n",
    "    checkpoint_dir=\"checkpoints/opponent\",\n",
    "    checkpoint_freq=config_opponent[\"checkpoint_freq\"],\n",
    "    save_best=True,\n",
    "    tags=[\"opponent\", \"sac\", \"basic_opponent\"],\n",
    "    notes=\"SAC training against BasicOpponent (strong)\",\n",
    ")\n",
    "\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Action space bounds: [{env.action_space.low[0]}, {env.action_space.high[0]}]\")\n",
    "print(f\"WandB run URL: {recorder.url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5efade",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felix/Studium/Reinforcement-Learning/exam/hockey-env/venv/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10 | Reward: -10.60 | W/L/D: 1/4/5 | Win Rate: 10.00%\n",
      "Episode 20 | Reward: -33.92 | W/L/D: 1/9/10 | Win Rate: 5.00%\n",
      "Episode 30 | Reward: 7.27 | W/L/D: 3/12/15 | Win Rate: 10.00%\n",
      "Episode 40 | Reward: -17.03 | W/L/D: 5/16/19 | Win Rate: 12.50%\n",
      "Checkpoint saved: checkpoints/opponent/checkpoint_step_50.pt (uploaded to WandB)\n",
      "Episode 50 | Reward: -5.72 | W/L/D: 7/20/23 | Win Rate: 14.00%\n",
      "Episode 60 | Reward: -3.95 | W/L/D: 7/26/27 | Win Rate: 11.67%\n",
      "Episode 70 | Reward: -17.13 | W/L/D: 9/31/30 | Win Rate: 12.86%\n",
      "Episode 80 | Reward: -17.26 | W/L/D: 9/38/33 | Win Rate: 11.25%\n",
      "Episode 90 | Reward: -17.17 | W/L/D: 13/44/33 | Win Rate: 14.44%\n",
      "Checkpoint saved: checkpoints/opponent/checkpoint_step_100.pt (uploaded to WandB)\n",
      "Episode 100 | Reward: -17.22 | W/L/D: 15/50/35 | Win Rate: 15.00%\n",
      "Episode 110 | Reward: 9.18 | W/L/D: 17/56/37 | Win Rate: 15.45%\n",
      "Episode 120 | Reward: -8.49 | W/L/D: 18/62/40 | Win Rate: 15.00%\n",
      "Episode 130 | Reward: 9.04 | W/L/D: 23/67/40 | Win Rate: 17.69%\n",
      "Episode 140 | Reward: 8.30 | W/L/D: 28/70/42 | Win Rate: 20.00%\n",
      "Checkpoint saved: checkpoints/opponent/checkpoint_step_150.pt (uploaded to WandB)\n",
      "Episode 150 | Reward: 8.84 | W/L/D: 34/73/43 | Win Rate: 22.67%\n",
      "Episode 160 | Reward: -16.78 | W/L/D: 37/79/44 | Win Rate: 23.12%\n",
      "Episode 170 | Reward: -17.42 | W/L/D: 39/84/47 | Win Rate: 22.94%\n",
      "Episode 180 | Reward: 5.54 | W/L/D: 43/90/47 | Win Rate: 23.89%\n",
      "Episode 190 | Reward: -22.22 | W/L/D: 46/93/51 | Win Rate: 24.21%\n",
      "Checkpoint saved: checkpoints/opponent/checkpoint_step_200.pt (uploaded to WandB)\n",
      "Episode 200 | Reward: 6.66 | W/L/D: 50/95/55 | Win Rate: 25.00%\n",
      "Episode 210 | Reward: -16.81 | W/L/D: 53/100/57 | Win Rate: 25.24%\n",
      "Episode 220 | Reward: -17.39 | W/L/D: 56/105/59 | Win Rate: 25.45%\n",
      "Episode 230 | Reward: -12.61 | W/L/D: 58/112/60 | Win Rate: 25.22%\n",
      "Episode 240 | Reward: 9.22 | W/L/D: 61/116/63 | Win Rate: 25.42%\n",
      "Checkpoint saved: checkpoints/opponent/checkpoint_step_250.pt (uploaded to WandB)\n",
      "Episode 250 | Reward: -6.90 | W/L/D: 64/120/66 | Win Rate: 25.60%\n",
      "Episode 260 | Reward: -13.12 | W/L/D: 65/126/69 | Win Rate: 25.00%\n",
      "Episode 270 | Reward: -23.71 | W/L/D: 69/130/71 | Win Rate: 25.56%\n",
      "Episode 280 | Reward: -17.12 | W/L/D: 73/134/73 | Win Rate: 26.07%\n",
      "Episode 290 | Reward: 4.06 | W/L/D: 77/139/74 | Win Rate: 26.55%\n",
      "Checkpoint saved: checkpoints/opponent/checkpoint_step_300.pt (uploaded to WandB)\n",
      "Episode 300 | Reward: 9.05 | W/L/D: 80/142/78 | Win Rate: 26.67%\n",
      "Episode 310 | Reward: 7.82 | W/L/D: 84/146/80 | Win Rate: 27.10%\n",
      "Episode 320 | Reward: -15.29 | W/L/D: 87/149/84 | Win Rate: 27.19%\n",
      "Episode 330 | Reward: 8.68 | W/L/D: 90/154/86 | Win Rate: 27.27%\n",
      "Episode 340 | Reward: -13.37 | W/L/D: 92/160/88 | Win Rate: 27.06%\n",
      "Checkpoint saved: checkpoints/opponent/checkpoint_step_350.pt (uploaded to WandB)\n",
      "Episode 350 | Reward: -18.80 | W/L/D: 95/164/91 | Win Rate: 27.14%\n",
      "Episode 360 | Reward: -15.72 | W/L/D: 98/169/93 | Win Rate: 27.22%\n",
      "Episode 370 | Reward: -15.89 | W/L/D: 100/173/97 | Win Rate: 27.03%\n",
      "Episode 380 | Reward: 8.53 | W/L/D: 105/176/99 | Win Rate: 27.63%\n",
      "Episode 390 | Reward: -1.85 | W/L/D: 108/180/102 | Win Rate: 27.69%\n",
      "Checkpoint saved: checkpoints/opponent/checkpoint_step_400.pt (uploaded to WandB)\n",
      "Episode 400 | Reward: -12.41 | W/L/D: 111/186/103 | Win Rate: 27.75%\n",
      "Episode 410 | Reward: -4.93 | W/L/D: 112/190/108 | Win Rate: 27.32%\n",
      "Episode 420 | Reward: -8.08 | W/L/D: 114/195/111 | Win Rate: 27.14%\n",
      "Episode 430 | Reward: 9.00 | W/L/D: 117/199/114 | Win Rate: 27.21%\n",
      "Episode 440 | Reward: 8.76 | W/L/D: 120/204/116 | Win Rate: 27.27%\n",
      "Checkpoint saved: checkpoints/opponent/checkpoint_step_450.pt (uploaded to WandB)\n",
      "Episode 450 | Reward: -13.89 | W/L/D: 124/208/118 | Win Rate: 27.56%\n",
      "Episode 460 | Reward: 8.41 | W/L/D: 132/210/118 | Win Rate: 28.70%\n",
      "Episode 470 | Reward: -12.93 | W/L/D: 135/215/120 | Win Rate: 28.72%\n",
      "Episode 480 | Reward: 9.37 | W/L/D: 138/218/124 | Win Rate: 28.75%\n",
      "Episode 490 | Reward: -14.40 | W/L/D: 141/222/127 | Win Rate: 28.78%\n",
      "Checkpoint saved: checkpoints/opponent/checkpoint_step_500.pt (uploaded to WandB)\n",
      "Episode 500 | Reward: 8.44 | W/L/D: 147/226/127 | Win Rate: 29.40%\n",
      "Episode 510 | Reward: -3.47 | W/L/D: 149/231/130 | Win Rate: 29.22%\n",
      "Episode 520 | Reward: -11.04 | W/L/D: 151/237/132 | Win Rate: 29.04%\n",
      "Episode 530 | Reward: 9.12 | W/L/D: 157/240/133 | Win Rate: 29.62%\n",
      "Episode 540 | Reward: -15.33 | W/L/D: 159/244/137 | Win Rate: 29.44%\n",
      "Checkpoint saved: checkpoints/opponent/checkpoint_step_550.pt (uploaded to WandB)\n",
      "Episode 550 | Reward: -1.17 | W/L/D: 163/249/138 | Win Rate: 29.64%\n",
      "Episode 560 | Reward: 7.64 | W/L/D: 168/252/140 | Win Rate: 30.00%\n",
      "Episode 570 | Reward: 9.22 | W/L/D: 173/256/141 | Win Rate: 30.35%\n",
      "Episode 580 | Reward: -17.13 | W/L/D: 176/261/143 | Win Rate: 30.34%\n",
      "Episode 590 | Reward: -14.54 | W/L/D: 179/266/145 | Win Rate: 30.34%\n",
      "Checkpoint saved: checkpoints/opponent/checkpoint_step_600.pt (uploaded to WandB)\n",
      "Episode 600 | Reward: 9.15 | W/L/D: 184/271/145 | Win Rate: 30.67%\n",
      "Episode 610 | Reward: 8.51 | W/L/D: 191/273/146 | Win Rate: 31.31%\n",
      "Episode 620 | Reward: 8.93 | W/L/D: 199/274/147 | Win Rate: 32.10%\n",
      "Episode 630 | Reward: 9.24 | W/L/D: 203/277/150 | Win Rate: 32.22%\n",
      "Episode 640 | Reward: 8.00 | W/L/D: 208/281/151 | Win Rate: 32.50%\n",
      "Checkpoint saved: checkpoints/opponent/checkpoint_step_650.pt (uploaded to WandB)\n",
      "Episode 650 | Reward: 9.05 | W/L/D: 213/285/152 | Win Rate: 32.77%\n",
      "Episode 660 | Reward: -1.53 | W/L/D: 218/290/152 | Win Rate: 33.03%\n",
      "Episode 670 | Reward: -13.59 | W/L/D: 222/295/153 | Win Rate: 33.13%\n",
      "Episode 680 | Reward: -7.00 | W/L/D: 224/298/158 | Win Rate: 32.94%\n",
      "Episode 690 | Reward: 8.81 | W/L/D: 230/301/159 | Win Rate: 33.33%\n",
      "Checkpoint saved: checkpoints/opponent/checkpoint_step_700.pt (uploaded to WandB)\n",
      "Episode 700 | Reward: -16.70 | W/L/D: 235/304/161 | Win Rate: 33.57%\n",
      "Episode 710 | Reward: 9.33 | W/L/D: 239/309/162 | Win Rate: 33.66%\n",
      "Episode 720 | Reward: 7.72 | W/L/D: 244/313/163 | Win Rate: 33.89%\n",
      "Episode 730 | Reward: -11.61 | W/L/D: 248/316/166 | Win Rate: 33.97%\n",
      "Episode 740 | Reward: 9.21 | W/L/D: 254/319/167 | Win Rate: 34.32%\n",
      "Checkpoint saved: checkpoints/opponent/checkpoint_step_750.pt (uploaded to WandB)\n",
      "Episode 750 | Reward: 7.89 | W/L/D: 259/324/167 | Win Rate: 34.53%\n",
      "Episode 760 | Reward: -13.15 | W/L/D: 265/327/168 | Win Rate: 34.87%\n",
      "Episode 770 | Reward: -17.16 | W/L/D: 269/331/170 | Win Rate: 34.94%\n",
      "Episode 780 | Reward: -13.62 | W/L/D: 274/336/170 | Win Rate: 35.13%\n",
      "Episode 790 | Reward: 8.24 | W/L/D: 279/341/170 | Win Rate: 35.32%\n",
      "Checkpoint saved: checkpoints/opponent/checkpoint_step_800.pt (uploaded to WandB)\n",
      "Episode 800 | Reward: 9.08 | W/L/D: 285/345/170 | Win Rate: 35.62%\n",
      "Episode 810 | Reward: 9.05 | W/L/D: 291/349/170 | Win Rate: 35.93%\n",
      "Episode 820 | Reward: 8.74 | W/L/D: 296/353/171 | Win Rate: 36.10%\n",
      "Episode 830 | Reward: 9.47 | W/L/D: 302/356/172 | Win Rate: 36.39%\n",
      "Episode 840 | Reward: -17.58 | W/L/D: 308/360/172 | Win Rate: 36.67%\n",
      "Checkpoint saved: checkpoints/opponent/checkpoint_step_850.pt (uploaded to WandB)\n",
      "Episode 850 | Reward: 8.20 | W/L/D: 315/362/173 | Win Rate: 37.06%\n",
      "Episode 860 | Reward: 8.93 | W/L/D: 321/365/174 | Win Rate: 37.33%\n",
      "Episode 870 | Reward: 9.52 | W/L/D: 327/368/175 | Win Rate: 37.59%\n",
      "Episode 880 | Reward: 8.68 | W/L/D: 329/374/177 | Win Rate: 37.39%\n",
      "Episode 890 | Reward: 9.39 | W/L/D: 335/377/178 | Win Rate: 37.64%\n",
      "Checkpoint saved: checkpoints/opponent/checkpoint_step_900.pt (uploaded to WandB)\n",
      "Episode 900 | Reward: 7.81 | W/L/D: 341/380/179 | Win Rate: 37.89%\n",
      "Episode 910 | Reward: -12.83 | W/L/D: 343/386/181 | Win Rate: 37.69%\n",
      "Episode 920 | Reward: 9.33 | W/L/D: 350/389/181 | Win Rate: 38.04%\n",
      "Episode 930 | Reward: 9.61 | W/L/D: 354/394/182 | Win Rate: 38.06%\n",
      "Episode 940 | Reward: 9.34 | W/L/D: 360/396/184 | Win Rate: 38.30%\n",
      "Checkpoint saved: checkpoints/opponent/checkpoint_step_950.pt (uploaded to WandB)\n",
      "Episode 950 | Reward: 9.23 | W/L/D: 364/402/184 | Win Rate: 38.32%\n",
      "Episode 960 | Reward: 3.38 | W/L/D: 372/404/184 | Win Rate: 38.75%\n",
      "Episode 970 | Reward: 8.97 | W/L/D: 378/408/184 | Win Rate: 38.97%\n",
      "Episode 980 | Reward: 9.50 | W/L/D: 385/411/184 | Win Rate: 39.29%\n",
      "Episode 990 | Reward: 5.47 | W/L/D: 389/414/187 | Win Rate: 39.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved: checkpoints/opponent/checkpoint_step_1000.pt (uploaded to WandB)\n",
      "Episode 1000 | Reward: 9.37 | W/L/D: 394/419/187 | Win Rate: 39.40%\n",
      "Finishing WandB run: opponent_training\n",
      "Total episodes: 1000\n",
      "Total steps: 117382\n",
      "Best rolling reward: -0.02\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>buffer/capacity</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>buffer/fill_ratio</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇▇▇██████████</td></tr><tr><td>buffer/size</td><td>▁▁▁▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇██████████</td></tr><tr><td>episode</td><td>▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▇▇▇▇█████</td></tr><tr><td>episode/closeness_to_puck</td><td>▇▇▄▅▅▆▅▇▇▇█▇█▆▁▅▅▆█▆▇▇█▇▇▇█▆█▇▇▇▇▇▇██▇█▆</td></tr><tr><td>episode/draws_total</td><td>▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▄▄▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇█████</td></tr><tr><td>episode/length</td><td>▁█▁▇█▄▁▃▄▁█▃▅▂▁▂█▃▇▁██▂▅▂▁▁▁▇▂▂▄▁▁▁▁█▄▄▁</td></tr><tr><td>episode/losses_total</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>episode/puck_direction</td><td>▇▃▃▁▁▅▂█▄▇▂▃▅▁▂▃▄▃▇▁▁▃▇▇▄██▇▁▃█▇▃▇▆▇▇█▇▂</td></tr><tr><td>episode/reward</td><td>▁▇▄▃▆▁██▂█▂▂██▃▃█▂█▁█▂▇█▄█▇██▂▁██▃▃▁▂█▃▂</td></tr><tr><td>+17</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>buffer/capacity</td><td>100000</td></tr><tr><td>buffer/fill_ratio</td><td>1</td></tr><tr><td>buffer/size</td><td>100000</td></tr><tr><td>episode</td><td>1000</td></tr><tr><td>episode/closeness_to_puck</td><td>-0.62867</td></tr><tr><td>episode/draws_total</td><td>187</td></tr><tr><td>episode/length</td><td>44</td></tr><tr><td>episode/losses_total</td><td>419</td></tr><tr><td>episode/puck_direction</td><td>0.05304</td></tr><tr><td>episode/reward</td><td>9.37133</td></tr><tr><td>+17</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">opponent_training</strong> at: <a href='https://wandb.ai/htwk-robots/hockey-sac/runs/10touplv' target=\"_blank\">https://wandb.ai/htwk-robots/hockey-sac/runs/10touplv</a><br> View project at: <a href='https://wandb.ai/htwk-robots/hockey-sac' target=\"_blank\">https://wandb.ai/htwk-robots/hockey-sac</a><br>Synced 4 W&B file(s), 0 media file(s), 40 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260118_130919-10touplv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete! Final W/L/D: 394/419/187\n"
     ]
    }
   ],
   "source": [
    "obs, info = env.reset(seed=42)\n",
    "obs = normalize_obs(obs)\n",
    "\n",
    "global_step = 0\n",
    "total_reward = 0\n",
    "wins = 0\n",
    "losses = 0\n",
    "draws = 0\n",
    "\n",
    "try:\n",
    "    for episode in range(config_opponent[\"training\"][\"num_episodes\"]):\n",
    "        episode_length = 0\n",
    "        last_info = info\n",
    "        \n",
    "        # Accumulators for training metrics\n",
    "        episode_metrics = {\n",
    "            \"actor_loss\": [],\n",
    "            \"critic_loss\": [],\n",
    "            \"alpha_loss\": [],\n",
    "            \"alpha\": [],\n",
    "            \"q1_mean\": [],\n",
    "            \"q2_mean\": [],\n",
    "        }\n",
    "        \n",
    "        # Accumulators for environment info rewards\n",
    "        episode_info_rewards = {\n",
    "            \"reward_closeness_to_puck\": 0.0,\n",
    "            \"reward_touch_puck\": 0.0,\n",
    "            \"reward_puck_direction\": 0.0,\n",
    "        }\n",
    "        \n",
    "        for step in range(config_opponent[\"training\"][\"episode_length\"]):\n",
    "            global_step += 1\n",
    "            episode_length += 1\n",
    "            \n",
    "            env.render()\n",
    "\n",
    "            # Sample action from SAC agent\n",
    "            action_1 = sac.act(obs)\n",
    "\n",
    "            # Get opponent action\n",
    "            obs_agent2 = env.obs_agent_two()\n",
    "            action_2 = player2.act(obs_agent2)\n",
    "\n",
    "            env_action = np.hstack([action_1, action_2])\n",
    "\n",
    "            # Take a step in the environment\n",
    "            next_obs, reward, terminated, truncated, info = env.step(env_action)\n",
    "            next_obs = normalize_obs(next_obs)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Accumulate info rewards\n",
    "            for key in episode_info_rewards:\n",
    "                if key in info:\n",
    "                    episode_info_rewards[key] += info[key]\n",
    "\n",
    "            buffer.add(obs, action_1, reward, next_obs, terminated)\n",
    "            obs = next_obs\n",
    "\n",
    "            # Update SAC and accumulate metrics\n",
    "            if buffer.is_ready(config_opponent[\"buffer\"][\"min_size\"]):\n",
    "                metrics = sac.update()\n",
    "                for key in episode_metrics:\n",
    "                    episode_metrics[key].append(metrics[key])\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                last_info = info\n",
    "                # Track game outcomes\n",
    "                if info.get(\"winner\", 0) == 1:\n",
    "                    wins += 1\n",
    "                elif info.get(\"winner\", 0) == -1:\n",
    "                    losses += 1\n",
    "                else:\n",
    "                    draws += 1\n",
    "                break\n",
    "\n",
    "        # Log accumulated training metrics at end of episode\n",
    "        if episode_metrics[\"actor_loss\"]:  # Only log if we had updates\n",
    "            recorder.log_update(\n",
    "                global_step=global_step,\n",
    "                actor_loss=np.mean(episode_metrics[\"actor_loss\"]),\n",
    "                critic_loss=np.mean(episode_metrics[\"critic_loss\"]),\n",
    "                alpha_loss=np.mean(episode_metrics[\"alpha_loss\"]),\n",
    "                alpha=np.mean(episode_metrics[\"alpha\"]),\n",
    "                q1_mean=np.mean(episode_metrics[\"q1_mean\"]),\n",
    "                q2_mean=np.mean(episode_metrics[\"q2_mean\"]),\n",
    "                extra_metrics={\n",
    "                    \"num_updates\": len(episode_metrics[\"actor_loss\"]),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # Log episode metrics with accumulated info rewards\n",
    "        is_best = recorder.log_episode(\n",
    "            episode=episode + 1,\n",
    "            reward=total_reward,\n",
    "            length=episode_length,\n",
    "            winner=last_info.get(\"winner\", 0),\n",
    "            info=episode_info_rewards,  # Pass accumulated rewards instead of last_info\n",
    "            extra_metrics={\n",
    "                \"wins_total\": wins,\n",
    "                \"losses_total\": losses,\n",
    "                \"draws_total\": draws,\n",
    "                \"win_rate\": wins / (episode + 1),\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Log buffer stats periodically\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            recorder.log_buffer(global_step, len(buffer), buffer.capacity)\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if (episode + 1) % config_opponent[\"training\"][\"checkpoint_freq\"] == 0:\n",
    "            recorder.save_checkpoint(sac, episode + 1)\n",
    "        \n",
    "        # Save best model\n",
    "        if is_best:\n",
    "            recorder.save_checkpoint(sac, episode + 1, is_best=True)\n",
    "        \n",
    "        # Print progress\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            print(f\"Episode {episode + 1} | Reward: {total_reward:.2f} | \"\n",
    "                  f\"W/L/D: {wins}/{losses}/{draws} | Win Rate: {wins/(episode+1):.2%}\")\n",
    "        \n",
    "        # Reset for next episode\n",
    "        total_reward = 0\n",
    "        obs, info = env.reset()\n",
    "\n",
    "finally:\n",
    "    # Always finish the recorder (uploads remaining data)\n",
    "    recorder.finish()\n",
    "    print(f\"Training complete! Final W/L/D: {wins}/{losses}/{draws}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0f8bab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finishing WandB run: opponent_training\n",
      "Total episodes: 8\n",
      "Total steps: 183\n",
      "Best rolling reward: -inf\n"
     ]
    }
   ],
   "source": [
    "recorder.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
